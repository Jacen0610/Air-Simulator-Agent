# C:/.../Air-Simulator-Py-Agent/train.py
import torch
import torch.optim as optim
import time
import os
from torch.utils.tensorboard import SummaryWriter

from environment import GoSimulatorEnv
from model import ActorCritic

# ==============================================================================
#                           è¶…å‚æ•°é…ç½® (Hyperparameters)
# ==============================================================================

# --- æ ¸å¿ƒè°ƒæ•´ï¼šç¨³å®šå­¦ä¹ çš„å…³é”® ---
LEARNING_RATE = 1e-4  # ä¿æŒè¾ƒä½çš„å­¦ä¹ ç‡
ENTROPY_COEFF = 0.01  # ç†µæ­£åˆ™åŒ–ç³»æ•°ï¼Œé¼“åŠ±æ¢ç´¢

# --- åŸºç¡€é…ç½® ---
GAMMA = 0.99
OBSERVATION_DIM = 6
ACTION_DIM = 3

# --- è®­ç»ƒæµç¨‹æ§åˆ¶ ---
NUM_EPISODES = 100
MAX_STEPS_PER_EPISODE = 13000

# --- æ¨¡å‹ä¿å­˜ä¸æ—¥å¿— ---
SAVE_INTERVAL = 10
MODEL_SAVE_DIR = "models"
LOG_DIR = "logs"
CONTINUE_FROM_EPISODE = 0


def train():
    # ==============================================================================
    #                                1. å‡†å¤‡é˜¶æ®µ (Setup Phase)
    # ==============================================================================
    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–ç¯å¢ƒå’Œæ™ºèƒ½ä½“...")

    # 1.1. åˆå§‹åŒ–ç¯å¢ƒå’Œæ—¥å¿—è®°å½•å™¨
    env = GoSimulatorEnv()
    writer = SummaryWriter(LOG_DIR)
    print(f"ğŸ“ æ—¥å¿—å°†ä¿å­˜åœ¨: {LOG_DIR}")

    # 1.2. é‡ç½®ç¯å¢ƒä¸€æ¬¡ï¼Œä»¥è·å–æ™ºèƒ½ä½“åˆ—è¡¨
    initial_observations = env.reset()
    if initial_observations is None:
        print("âŒ æ— æ³•ä»ç¯å¢ƒä¸­è·å–åˆå§‹çŠ¶æ€ï¼Œè®­ç»ƒç»ˆæ­¢ã€‚")
        env.close()
        return

    agent_ids = list(initial_observations.keys())
    print(f"ğŸ¤– å‘ç° {len(agent_ids)} ä¸ªæ™ºèƒ½ä½“: {agent_ids}")

    # 1.3. åˆ›å»ºæ¨¡å‹
    agents = {
        agent_id: ActorCritic(OBSERVATION_DIM, ACTION_DIM)
        for agent_id in agent_ids
    }

    # ç¡®ä¿æ¨¡å‹ä¿å­˜ç›®å½•å­˜åœ¨
    if not os.path.exists(MODEL_SAVE_DIR):
        os.makedirs(MODEL_SAVE_DIR)
        print(f"ğŸ“‚ åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•: {MODEL_SAVE_DIR}")

    # 1.4. æ˜ç¡®çš„é€»è¾‘åˆ†æ”¯ï¼šåŠ è½½æ¨¡å‹ æˆ– å¼€å§‹æ–°è®­ç»ƒ
    if CONTINUE_FROM_EPISODE > 0:
        print(f"\nğŸ”„ æ¨¡å¼: ç»§ç»­è®­ç»ƒã€‚å°è¯•ä» Episode {CONTINUE_FROM_EPISODE} åŠ è½½æ¨¡å‹...")
        for agent_id, model in agents.items():
            load_path = os.path.join(MODEL_SAVE_DIR, f"agent_{agent_id}_episode_{CONTINUE_FROM_EPISODE}.pth")
            if os.path.exists(load_path):
                model.load_state_dict(torch.load(load_path))
                model.train()
                print(f"  - âœ… æˆåŠŸåŠ è½½æ™ºèƒ½ä½“ {agent_id} çš„æ¨¡å‹ã€‚")
            else:
                print(f"  - âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°æ™ºèƒ½ä½“ {agent_id} çš„æ¨¡å‹æ–‡ä»¶ï¼Œå°†ä½¿ç”¨æ–°åˆå§‹åŒ–çš„æ¨¡å‹ã€‚")
    else:
        print("\nâœ¨ æ¨¡å¼: å¼€å§‹æ–°çš„è®­ç»ƒã€‚")

    # 1.5. åˆ›å»ºä¼˜åŒ–å™¨
    optimizers = {
        agent_id: optim.Adam(agents[agent_id].parameters(), lr=LEARNING_RATE)
        for agent_id in agent_ids
    }

    print("\nâœ… å‡†å¤‡é˜¶æ®µå®Œæˆï¼Œå¼€å§‹è®­ç»ƒå¾ªç¯...")
    # ==============================================================================
    #                                2. è®­ç»ƒé˜¶æ®µ (Training Phase)
    # ==============================================================================
    observations = initial_observations
    start_time = time.time()
    for episode in range(CONTINUE_FROM_EPISODE, NUM_EPISODES):
        if episode > CONTINUE_FROM_EPISODE:
            observations = env.reset()
            if observations is None:
                print(f"âŒ åœ¨ Episode {episode + 1} å¼€å§‹æ—¶é‡ç½®å¤±è´¥ï¼Œè®­ç»ƒç»ˆæ­¢ã€‚")
                break

        total_episode_rewards = {agent_id: 0 for agent_id in agents.keys()}
        is_done = False

        # 2.1. å•ä¸ª Episode çš„ Step å¾ªç¯
        for step in range(MAX_STEPS_PER_EPISODE):
            actions_to_take = {}
            log_probs = {}
            state_values = {}
            entropies = {}  # <--- ç”¨äºå­˜å‚¨ç†µ

            # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ
            for agent_id, obs in observations.items():
                action_dist, state_value = agents[agent_id](obs)
                action = action_dist.sample()

                actions_to_take[agent_id] = action.item()
                log_probs[agent_id] = action_dist.log_prob(action)
                state_values[agent_id] = state_value
                entropies[agent_id] = action_dist.entropy()  # <--- è®¡ç®—ç†µ

            # ä¸ç¯å¢ƒäº¤äº’
            next_observations, rewards, dones, _ = env.step(actions_to_take)

            if next_observations is None:
                is_done = True
                print("âŒ Step å¤±è´¥ï¼Œæå‰ç»“æŸæœ¬è½® Episodeã€‚")

            if not is_done:
                # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“è®¡ç®—æŸå¤±å¹¶æ›´æ–°ç½‘ç»œ
                for agent_id in agents.keys():
                    _, next_state_value = agents[agent_id](next_observations[agent_id])
                    advantage = rewards[agent_id] + GAMMA * next_state_value - state_values[agent_id]

                    actor_loss = -log_probs[agent_id] * advantage.detach()
                    critic_loss = advantage.pow(2)

                    # =========================================================
                    #               [æ ¸å¿ƒä¿®å¤] é‡æ–°åŠ å…¥ç†µæ­£åˆ™åŒ–
                    # =========================================================
                    entropy_loss = -ENTROPY_COEFF * entropies[agent_id]
                    total_loss = actor_loss + critic_loss + entropy_loss
                    # =========================================================

                    optimizers[agent_id].zero_grad()
                    total_loss.backward()
                    optimizers[agent_id].step()

                    total_episode_rewards[agent_id] += rewards[agent_id]

                if any(dones.values()):
                    is_done = True

                observations = next_observations

            if is_done:
                break

            time.sleep(0.5)

        # 2.2. æ—¥å¿—è®°å½•å’Œæ¨¡å‹ä¿å­˜ (ä¸å˜)
        avg_reward = sum(total_episode_rewards.values()) / len(agents) if agents else 0
        writer.add_scalar('Reward/Average_Reward', avg_reward, episode)
        for agent_id, reward in total_episode_rewards.items():
            writer.add_scalar(f'Reward/Agent_{agent_id}', reward, episode)

        elapsed_time = time.time() - start_time
        print(f"Episode {episode + 1}/{NUM_EPISODES} | Avg Reward: {avg_reward:.2f} | Time: {elapsed_time:.2f}s")

        if (episode + 1) % SAVE_INTERVAL == 0:
            print(f"\nğŸ’¾ Episode {episode + 1}: æ­£åœ¨ä¿å­˜æ¨¡å‹...")
            for agent_id, model in agents.items():
                save_path = os.path.join(MODEL_SAVE_DIR, f"agent_{agent_id}_episode_{episode + 1}.pth")
                torch.save(model.state_dict(), save_path)
            print(f"âœ… æ‰€æœ‰æ¨¡å‹å·²ä¿å­˜è‡³ '{MODEL_SAVE_DIR}' ç›®å½•ã€‚\n")

    env.close()
    writer.close()


if __name__ == '__main__':
    train()