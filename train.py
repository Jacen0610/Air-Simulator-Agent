# C:/.../Air-Simulator-Py-Agent/train.py
import torch
import torch.optim as optim
import time
import os
from torch.utils.tensorboard import SummaryWriter

from environment import GoSimulatorEnv
from model import ActorCritic

# ==============================================================================
#                           è¶…å‚æ•°é…ç½® (Hyperparameters)
# ==============================================================================

# --- æ ¸å¿ƒè°ƒæ•´ï¼šç¨³å®šå­¦ä¹ çš„å…³é”® ---
LEARNING_RATE = 1e-4  # ä¿æŒè¾ƒä½çš„å­¦ä¹ ç‡
ENTROPY_COEFF = 0.01  # ç†µæ­£åˆ™åŒ–ç³»æ•°ï¼Œé¼“åŠ±æ¢ç´¢

# --- åŸºç¡€é…ç½® ---
GAMMA = 0.99
OBSERVATION_DIM = 6
ACTION_DIM = 3

# --- è®­ç»ƒæµç¨‹æ§åˆ¶ ---
NUM_EPISODES = 25
MAX_STEPS_PER_EPISODE = 5000
# =========================================================
#               [æ ¸å¿ƒä¿®æ”¹] æ–°å¢æ›´æ–°é¢‘ç‡æ§åˆ¶
# =========================================================
UPDATE_EVERY_STEPS = 500  # æ¯éš”500ä¸ªstepæ›´æ–°ä¸€æ¬¡ç½‘ç»œ
# =========================================================

# --- æ¨¡å‹ä¿å­˜ä¸æ—¥å¿— ---
SAVE_INTERVAL = 5
MODEL_SAVE_DIR = "models"
LOG_DIR = "logs"
CONTINUE_FROM_EPISODE = 0


def train():
    # ==============================================================================
    #                                1. å‡†å¤‡é˜¶æ®µ (Setup Phase)
    # ==============================================================================
    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–ç¯å¢ƒå’Œæ™ºèƒ½ä½“...")

    # 1.1. åˆå§‹åŒ–ç¯å¢ƒå’Œæ—¥å¿—è®°å½•å™¨
    env = GoSimulatorEnv()
    writer = SummaryWriter(LOG_DIR)
    print(f"ğŸ“ æ—¥å¿—å°†ä¿å­˜åœ¨: {LOG_DIR}")

    # 1.2. é‡ç½®ç¯å¢ƒä¸€æ¬¡ï¼Œä»¥è·å–æ™ºèƒ½ä½“åˆ—è¡¨
    initial_observations = env.reset()
    if initial_observations is None:
        print("âŒ æ— æ³•ä»ç¯å¢ƒä¸­è·å–åˆå§‹çŠ¶æ€ï¼Œè®­ç»ƒç»ˆæ­¢ã€‚")
        env.close()
        return

    agent_ids = list(initial_observations.keys())
    print(f"ğŸ¤– å‘ç° {len(agent_ids)} ä¸ªæ™ºèƒ½ä½“: {agent_ids}")

    # 1.3. åˆ›å»ºæ¨¡å‹
    agents = {
        agent_id: ActorCritic(OBSERVATION_DIM, ACTION_DIM)
        for agent_id in agent_ids
    }

    # ç¡®ä¿æ¨¡å‹ä¿å­˜ç›®å½•å­˜åœ¨
    if not os.path.exists(MODEL_SAVE_DIR):
        os.makedirs(MODEL_SAVE_DIR)
        print(f"ğŸ“‚ åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•: {MODEL_SAVE_DIR}")

    # 1.4. æ˜ç¡®çš„é€»è¾‘åˆ†æ”¯ï¼šåŠ è½½æ¨¡å‹ æˆ– å¼€å§‹æ–°è®­ç»ƒ (é€»è¾‘ä¿æŒä¸å˜)
    if CONTINUE_FROM_EPISODE > 0:
        print(f"\nğŸ”„ æ¨¡å¼: ç»§ç»­è®­ç»ƒã€‚å°è¯•ä» Episode {CONTINUE_FROM_EPISODE} åŠ è½½æ¨¡å‹...")
        for agent_id, model in agents.items():
            load_path = os.path.join(MODEL_SAVE_DIR, f"agent_{agent_id}_episode_{CONTINUE_FROM_EPISODE}.pth")
            if os.path.exists(load_path):
                model.load_state_dict(torch.load(load_path))
                model.train()
                print(f"  - âœ… æˆåŠŸåŠ è½½æ™ºèƒ½ä½“ {agent_id} çš„æ¨¡å‹ã€‚")
            else:
                print(f"  - âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°æ™ºèƒ½ä½“ {agent_id} çš„æ¨¡å‹æ–‡ä»¶ï¼Œå°†ä½¿ç”¨æ–°åˆå§‹åŒ–çš„æ¨¡å‹ã€‚")
    else:
        print("\nâœ¨ æ¨¡å¼: å¼€å§‹æ–°çš„è®­ç»ƒã€‚")

    # 1.5. åˆ›å»ºä¼˜åŒ–å™¨
    optimizers = {
        agent_id: optim.Adam(agents[agent_id].parameters(), lr=LEARNING_RATE)
        for agent_id in agent_ids
    }

    print("\nâœ… å‡†å¤‡é˜¶æ®µå®Œæˆï¼Œå¼€å§‹è®­ç»ƒå¾ªç¯...")
    # ==============================================================================
    #                                2. è®­ç»ƒé˜¶æ®µ (Training Phase)
    # ==============================================================================
    observations = initial_observations
    start_time = time.time()
    for episode in range(CONTINUE_FROM_EPISODE, NUM_EPISODES):
        if episode > CONTINUE_FROM_EPISODE:
            observations = env.reset()
            if observations is None:
                print(f"âŒ åœ¨ Episode {episode + 1} å¼€å§‹æ—¶é‡ç½®å¤±è´¥ï¼Œè®­ç»ƒç»ˆæ­¢ã€‚")
                break

        total_episode_rewards = {agent_id: 0 for agent_id in agents.keys()}
        is_done = False

        # =========================================================
        #      [æ ¸å¿ƒä¿®æ”¹] ä¸ºæ¯ä¸ªå›åˆåˆå§‹åŒ–ç»éªŒç¼“å†²åŒº
        # =========================================================
        batch_log_probs = {agent_id: [] for agent_id in agents.keys()}
        batch_state_values = {agent_id: [] for agent_id in agents.keys()}
        batch_rewards = {agent_id: [] for agent_id in agents.keys()}
        batch_entropies = {agent_id: [] for agent_id in agents.keys()}
        # =========================================================

        # 2.1. å•ä¸ª Episode çš„ Step å¾ªç¯
        for step in range(MAX_STEPS_PER_EPISODE):
            actions_to_take = {}

            # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œå¹¶å­˜å‚¨ç»éªŒ
            for agent_id, obs in observations.items():
                action_dist, state_value = agents[agent_id](obs)
                action = action_dist.sample()

                actions_to_take[agent_id] = action.item()

                # å°†ç»éªŒå­˜å…¥ç¼“å†²åŒº
                batch_log_probs[agent_id].append(action_dist.log_prob(action))
                batch_state_values[agent_id].append(state_value)
                batch_entropies[agent_id].append(action_dist.entropy())

            # ä¸ç¯å¢ƒäº¤äº’
            next_observations, rewards, dones, _ = env.step(actions_to_take)

            # ç´¯åŠ æ¯ä¸€æ­¥çš„å¥–åŠ±åˆ°å›åˆæ€»å¥–åŠ±
            for agent_id, r in rewards.items():
                total_episode_rewards[agent_id] += r
                batch_rewards[agent_id].append(r)  # å°†å¥–åŠ±ä¹Ÿå­˜å…¥ç¼“å†²åŒº

            if next_observations is None:
                is_done = True
                print("âŒ Step å¤±è´¥ï¼Œæå‰ç»“æŸæœ¬è½® Episodeã€‚")
            else:
                observations = next_observations

            if any(dones.values()):
                is_done = True

            # =========================================================
            #      [æ ¸å¿ƒä¿®æ”¹] æ¯ 500 æ­¥æˆ–åœ¨å›åˆç»“æŸæ—¶ï¼Œæ‰§è¡Œä¸€æ¬¡æ›´æ–°
            # =========================================================
            if (step + 1) % UPDATE_EVERY_STEPS == 0 or is_done:
                # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“è®¡ç®—æŸå¤±å¹¶æ›´æ–°ç½‘ç»œ
                for agent_id in agents.keys():
                    # å¦‚æœç¼“å†²åŒºä¸ºç©ºï¼Œåˆ™è·³è¿‡æ­¤æ™ºèƒ½ä½“çš„æ›´æ–°
                    if not batch_rewards[agent_id]:
                        continue

                    # è®¡ç®— N-Step çš„å›æŠ¥
                    _, next_state_value = agents[agent_id](observations[agent_id])
                    if is_done:
                        next_state_value = torch.tensor([0.0])  # å¦‚æœå›åˆç»“æŸï¼Œæœªæ¥ä»·å€¼ä¸º0

                    # ä»åå¾€å‰è®¡ç®—æŠ˜æ‰£å›æŠ¥
                    returns = []
                    discounted_reward = next_state_value
                    for r in reversed(batch_rewards[agent_id]):
                        discounted_reward = r + GAMMA * discounted_reward
                        returns.insert(0, discounted_reward)

                    # è½¬æ¢æˆå¼ é‡
                    returns = torch.stack(returns)
                    log_probs_tensor = torch.stack(batch_log_probs[agent_id])
                    state_values_tensor = torch.cat(batch_state_values[agent_id]).squeeze()
                    entropies_tensor = torch.stack(batch_entropies[agent_id])

                    # è®¡ç®—ä¼˜åŠ¿
                    advantage = returns - state_values_tensor

                    # è®¡ç®—æŸå¤±
                    actor_loss = -(log_probs_tensor * advantage.detach()).mean()
                    critic_loss = advantage.pow(2).mean()
                    entropy_loss = -ENTROPY_COEFF * entropies_tensor.mean()
                    total_loss = actor_loss + critic_loss + entropy_loss

                    # æ›´æ–°ç½‘ç»œ
                    optimizers[agent_id].zero_grad()
                    total_loss.backward()
                    optimizers[agent_id].step()

                # æ›´æ–°åæ¸…ç©ºæ‰€æœ‰ç¼“å†²åŒºï¼Œä¸ºä¸‹ä¸€ä¸ªæ‰¹æ¬¡åšå‡†å¤‡
                batch_log_probs = {agent_id: [] for agent_id in agents.keys()}
                batch_state_values = {agent_id: [] for agent_id in agents.keys()}
                batch_rewards = {agent_id: [] for agent_id in agents.keys()}
                batch_entropies = {agent_id: [] for agent_id in agents.keys()}
            # =========================================================

            if is_done:
                break

            time.sleep(0.5)

        # 2.2. æ—¥å¿—è®°å½•å’Œæ¨¡å‹ä¿å­˜ (ä¸å˜)
        avg_reward = sum(total_episode_rewards.values()) / len(agents) if agents else 0
        writer.add_scalar('Reward/Average_Reward', avg_reward, episode)
        for agent_id, reward in total_episode_rewards.items():
            writer.add_scalar(f'Reward/Agent_{agent_id}', reward, episode)

        elapsed_time = time.time() - start_time
        print(f"Episode {episode + 1}/{NUM_EPISODES} | Avg Reward: {avg_reward:.2f} | Time: {elapsed_time:.2f}s")

        if (episode + 1) % SAVE_INTERVAL == 0:
            print(f"\nğŸ’¾ Episode {episode + 1}: æ­£åœ¨ä¿å­˜æ¨¡å‹...")
            for agent_id, model in agents.items():
                save_path = os.path.join(MODEL_SAVE_DIR, f"agent_{agent_id}_episode_{episode + 1}.pth")
                torch.save(model.state_dict(), save_path)
            print(f"âœ… æ‰€æœ‰æ¨¡å‹å·²ä¿å­˜è‡³ '{MODEL_SAVE_DIR}' ç›®å½•ã€‚\n")

    env.close()
    writer.close()


if __name__ == '__main__':
    train()