# C:/.../Air-Simulator-Py-Agent/train.py
import torch
import torch.optim as optim
import time
import os

from environment import GoSimulatorEnv
from model import ActorCritic

# --- è¶…å‚æ•° ---
OBSERVATION_DIM = 6
ACTION_DIM = 3
LEARNING_RATE = 0.001
GAMMA = 0.99

NUM_EPISODES = 50
MAX_STEPS_PER_EPISODE = 13000

# --- æ¨¡å‹ä¿å­˜ä¸åŠ è½½ç›¸å…³çš„è¶…å‚æ•° ---
SAVE_INTERVAL = 10
MODEL_SAVE_DIR = "models"
CONTINUE_FROM_EPISODE = 0


def train():
    # 1. åˆå§‹åŒ–ç¯å¢ƒ (ç°åœ¨è¿™æ­¥éå¸¸å¿«ï¼Œåªå»ºç«‹è¿æ¥)
    env = GoSimulatorEnv()

    # ç¡®ä¿æ¨¡å‹ä¿å­˜ç›®å½•å­˜åœ¨
    if not os.path.exists(MODEL_SAVE_DIR):
        os.makedirs(MODEL_SAVE_DIR)
        print(f"ğŸ“‚ åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•: {MODEL_SAVE_DIR}")

    # 2. **[æ ¸å¿ƒä¿®æ”¹]** åœ¨è®­ç»ƒå¼€å§‹å‰ï¼Œæˆ‘ä»¬è¿˜ä¸çŸ¥é“æœ‰å“ªäº›æ™ºèƒ½ä½“
    agents = {}
    optimizers = {}

    # 3. ä¸»è®­ç»ƒå¾ªç¯
    for episode in range(CONTINUE_FROM_EPISODE, NUM_EPISODES):
        # 3.1. åœ¨æ¯ä¸ªå›åˆå¼€å§‹æ—¶é‡ç½®ç¯å¢ƒ
        # è¿™æ˜¯è·å–æ™ºèƒ½ä½“åˆ—è¡¨å’Œåˆå§‹è§‚æµ‹çš„å”¯ä¸€åœ°æ–¹
        observations = env.reset()
        if observations is None:
            print("âŒ æ— æ³•ä»ç¯å¢ƒä¸­é‡ç½®ï¼Œè®­ç»ƒç»ˆæ­¢ã€‚")
            break

        # 3.2. **[æ ¸å¿ƒä¿®æ”¹]** å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªå›åˆï¼Œç°åœ¨æˆ‘ä»¬çŸ¥é“äº†æ™ºèƒ½ä½“åˆ—è¡¨ï¼Œå¯ä»¥åˆ›å»ºæ¨¡å‹äº†
        if episode == 0 and not agents:
            agent_ids = list(observations.keys())
            print(f"ğŸ¤– å‘ç° {len(agent_ids)} ä¸ªæ™ºèƒ½ä½“: {agent_ids}")

            agents = {
                agent_id: ActorCritic(OBSERVATION_DIM, ACTION_DIM)
                for agent_id in agent_ids
            }

            # å¦‚æœæ˜¯ç»§ç»­è®­ç»ƒï¼Œåˆ™åŠ è½½æ¨¡å‹
            if CONTINUE_FROM_EPISODE > 0:
                print(f"\nğŸ”„ å°è¯•ä» Episode {CONTINUE_FROM_EPISODE} ç»§ç»­è®­ç»ƒ...")
                # ... (åŠ è½½æ¨¡å‹çš„é€»è¾‘ä¸å˜) ...
                for agent_id, model in agents.items():
                    load_path = os.path.join(MODEL_SAVE_DIR, f"agent_{agent_id}_episode_{CONTINUE_FROM_EPISODE}.pth")
                    if os.path.exists(load_path):
                        model.load_state_dict(torch.load(load_path))
                        model.train()
                        print(f"  - âœ… æˆåŠŸåŠ è½½æ™ºèƒ½ä½“ {agent_id} çš„æ¨¡å‹ã€‚")
                    else:
                        print(f"  - âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°æ™ºèƒ½ä½“ {agent_id} çš„æ¨¡å‹æ–‡ä»¶ã€‚")

            optimizers = {
                agent_id: optim.Adam(agents[agent_id].parameters(), lr=LEARNING_RATE)
                for agent_id in agent_ids
            }

        total_episode_rewards = {agent_id: 0 for agent_id in agents.keys()}
        is_done = False

        # 3.3. å•ä¸ª Episode çš„ Step å¾ªç¯
        for step in range(MAX_STEPS_PER_EPISODE):
            # ... (å†…éƒ¨çš„ step é€»è¾‘å®Œå…¨ä¸å˜) ...
            actions_to_take = {}
            log_probs = {}
            state_values = {}

            for agent_id, obs in observations.items():
                action_dist, state_value = agents[agent_id](obs)
                action = action_dist.sample()
                actions_to_take[agent_id] = action.item()
                log_probs[agent_id] = action_dist.log_prob(action)
                state_values[agent_id] = state_value

            next_observations, rewards, dones, _ = env.step(actions_to_take)

            if next_observations is None:  # æ£€æŸ¥ gRPC é”™è¯¯
                is_done = True
                print("âŒ Step å¤±è´¥ï¼Œæå‰ç»“æŸæœ¬è½® Episodeã€‚")

            if not is_done:
                for agent_id in agents.keys():
                    _, next_state_value = agents[agent_id](next_observations[agent_id])
                    advantage = rewards[agent_id] + GAMMA * next_state_value - state_values[agent_id]
                    actor_loss = -log_probs[agent_id] * advantage.detach()
                    critic_loss = advantage.pow(2)
                    total_loss = actor_loss + critic_loss
                    optimizers[agent_id].zero_grad()
                    total_loss.backward()
                    optimizers[agent_id].step()
                    total_episode_rewards[agent_id] += rewards[agent_id]

                if any(dones.values()):
                    print(f"ğŸ Episode {episode + 1} åœ¨ç¬¬ {step + 1} æ­¥ç”±ç¯å¢ƒæŠ¥å‘Šå®Œæˆã€‚")
                    is_done = True

                observations = next_observations

            if is_done:
                break

            time.sleep(0.5)

        # ... (ä¿å­˜æ¨¡å‹å’Œæ‰“å°å¥–åŠ±çš„é€»è¾‘ä¸å˜) ...
        if (episode + 1) % SAVE_INTERVAL == 0:
            print(f"\nğŸ’¾ Episode {episode + 1}: æ­£åœ¨ä¿å­˜æ¨¡å‹...")
            for agent_id, model in agents.items():
                save_path = os.path.join(MODEL_SAVE_DIR, f"agent_{agent_id}_episode_{episode + 1}.pth")
                torch.save(model.state_dict(), save_path)
            print(f"âœ… æ‰€æœ‰æ¨¡å‹å·²ä¿å­˜è‡³ '{MODEL_SAVE_DIR}' ç›®å½•ã€‚\n")

        avg_reward = sum(total_episode_rewards.values()) / len(agents) if agents else 0
        print(f"Episode {episode + 1}/{NUM_EPISODES}, Average Reward: {avg_reward:.2f}")

    env.close()


if __name__ == '__main__':
    train()
